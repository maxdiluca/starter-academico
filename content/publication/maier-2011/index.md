---
# Documentation: https://sourcethemes.com/academic/docs/managing-content/

title: Audiovisual asynchrony detection in human speech.
subtitle: ''
summary: ''
authors:
- Joost X Maier
- Massimiliano Di Luca
- Uta Noppeney
tags:
- 'multisensory integration'
- 'spectral'
- 'speech'
- 'synchrony judgment'
- 'temporal order judgment'
categories: []
date: '2011-01-01'
lastmod: 2021-04-16T20:49:18+02:00
featured: false
draft: false

# Featured image
# To use, add an image named `featured.jpg/png` to your page's folder.
# Focal points: Smart, Center, TopLeft, Top, TopRight, Left, Right, BottomLeft, Bottom, BottomRight.
image:
  caption: ''
  focal_point: ''
  preview_only: false

# Projects (optional).
#   Associate this post with one or more of your projects.
#   Simply enter your project's folder or file name without extension.
#   E.g. `projects = ["internal-project"]` references `content/project/deep-learning/index.md`.
#   Otherwise, set `projects = []`.
projects: []
publishDate: '2021-04-16T18:49:18.414460Z'
publication_types:
- '2'
abstract: Combining information from the visual and auditory senses can greatly enhance
  intelligibility of natural speech. Integration of audiovisual speech signals is
  robust even when temporal offsets are present between the component signals. In
  the present study, we characterized the temporal integration window for speech and
  nonspeech stimuli with similar spectrotemporal structure to investigate to what
  extent humans have adapted to the specific characteristics of natural audiovisual
  speech. We manipulated spectrotemporal structure of the auditory signal, stimulus
  length, and task context. Results indicate that the temporal integration window
  is narrower and more asymmetric for speech than for nonspeech signals. When perceiving
  audiovisual speech, subjects tolerate visual leading asynchronies, but are nevertheless
  very sensitive to auditory leading asynchronies that are less likely to occur in
  natural speech. Thus, speech perception may be fine-tuned to the natural statistics
  of audiovisual speech, where facial movements always occur before acoustic speech
  articulation.
publication: '*Journal of experimental psychology. Human perception and performance*'
doi: 10.1037/a0019952
---
{{< paper_badges "10.1037/a0019952" >}}
